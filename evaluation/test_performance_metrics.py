#!/usr/bin/env python3
"""
Script para testar as m√©tricas de performance implementadas.
"""

import asyncio
import os
import sys
import json
from datetime import datetime

# Adiciona o diret√≥rio raiz ao path
root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, root_dir)

from api.services import AgentService
from api.config import OPENAI_API_KEY

class PerformanceTester:
    def __init__(self):
        self.api_key = OPENAI_API_KEY
        if not self.api_key:
            # Tentar pegar da vari√°vel de ambiente
            self.api_key = os.getenv("OPENAI_API_KEY")
            if not self.api_key:
                print("‚ö†Ô∏è OPENAI_API_KEY n√£o encontrada. Usando chave dummy para teste de estrutura.")
                self.api_key = "sk-dummy-key-for-testing"
        
        self.agent_service = AgentService(self.api_key)
    
    async def test_performance_metrics(self):
        """Testa as m√©tricas de performance implementadas"""
        
        print("üöÄ TESTE DE M√âTRICAS DE PERFORMANCE")
        print("=" * 50)
        
        test_messages = [
            "Quero trocar meu assento no voo JJ1234 para o assento 12A",
            "Qual √© o status do voo JJ1234?",
            "Preciso cancelar meu voo",
            "Quantas malas posso levar?",
        ]
        
        results = []
        
        for i, message in enumerate(test_messages, 1):
            print(f"\nüó£Ô∏è TESTE {i}: {message}")
            print("-" * 40)
            
            try:
                # Processar mensagem com m√©tricas
                result = await self.agent_service.process_message(message)
                
                # Extrair m√©tricas
                success = result.get('success', False)
                response = result.get('response', 'Sem resposta')
                agent_name = result.get('agent_name', 'Unknown')
                response_time = result.get('response_time_seconds', 0)
                performance_metrics = result.get('performance_metrics', {})
                
                print(f"‚úÖ Sucesso: {success}")
                print(f"ü§ñ Agente: {agent_name}")
                print(f"‚è±Ô∏è Tempo: {response_time}s")
                print(f"üìä M√©tricas: {json.dumps(performance_metrics, indent=2, ensure_ascii=False)}")
                print(f"üí¨ Resposta: {response[:100]}...")
                
                # An√°lise de performance
                self._analyze_performance(response_time, agent_name, len(message))
                
                results.append({
                    'test_number': i,
                    'message': message,
                    'success': success,
                    'agent_name': agent_name,
                    'response_time': response_time,
                    'performance_metrics': performance_metrics,
                    'response_preview': response[:100]
                })
                
            except Exception as e:
                print(f"‚ùå Erro no teste {i}: {e}")
                results.append({
                    'test_number': i,
                    'message': message,
                    'error': str(e)
                })
            
            # Pequena pausa entre testes
            await asyncio.sleep(1)
        
        # An√°lise final
        print("\nüìà AN√ÅLISE FINAL")
        print("=" * 50)
        self._final_performance_analysis(results)
        
        return results
    
    def _analyze_performance(self, response_time: float, agent_name: str, message_length: int):
        """Analisa a performance individual de uma resposta"""
        
        if response_time < 2.0:
            print(f"üü¢ Performance EXCELENTE ({response_time}s)")
        elif response_time < 5.0:
            print(f"üü° Performance BOA ({response_time}s)")
        elif response_time < 10.0:
            print(f"üü† Performance MODERADA ({response_time}s)")
        else:
            print(f"üî¥ Performance LENTA ({response_time}s)")
        
        # An√°lise por agente
        if "Agent" in agent_name and response_time > 8.0:
            print(f"‚ö†Ô∏è {agent_name} pode precisar de otimiza√ß√£o")
        
        # An√°lise por tamanho da mensagem
        if message_length > 100 and response_time > 6.0:
            print(f"üìù Mensagem longa ({message_length} chars) pode estar afetando performance")
    
    def _final_performance_analysis(self, results):
        """An√°lise final de todas as m√©tricas"""
        
        successful_tests = [r for r in results if r.get('success', False)]
        failed_tests = [r for r in results if not r.get('success', False)]
        
        if not successful_tests:
            print("‚ùå Nenhum teste bem-sucedido para analisar")
            return
        
        # Estat√≠sticas de tempo
        response_times = [r['response_time'] for r in successful_tests]
        avg_time = sum(response_times) / len(response_times)
        max_time = max(response_times)
        min_time = min(response_times)
        
        print("‚è±Ô∏è TEMPO DE RESPOSTA:")
        print(f"   - M√©dia: {avg_time:.3f}s")
        print(f"   - M√°ximo: {max_time:.3f}s")
        print(f"   - M√≠nimo: {min_time:.3f}s")
        
        # Estat√≠sticas por agente
        agents_used = {}
        for r in successful_tests:
            agent = r['agent_name']
            if agent in agents_used:
                agents_used[agent].append(r['response_time'])
            else:
                agents_used[agent] = [r['response_time']]
        
        print("\nü§ñ PERFORMANCE POR AGENTE:")
        for agent, times in agents_used.items():
            avg_agent_time = sum(times) / len(times)
            print(f"   - {agent}: {avg_agent_time:.3f}s (m√©dia de {len(times)} teste(s))")
        
        # An√°lise de erros
        if failed_tests:
            print(f"\n‚ùå ERROS ({len(failed_tests)} teste(s)):")
            for r in failed_tests:
                print(f"   - Teste {r['test_number']}: {r.get('error', 'Erro desconhecido')}")
        
        # Recomenda√ß√µes
        print("\nüí° RECOMENDA√á√ïES:")
        if avg_time > 8.0:
            print("   - Considerar otimiza√ß√£o geral do sistema (tempo m√©dio > 8s)")
        if max_time > 15.0:
            print("   - Investigar gargalos espec√≠ficos (tempo m√°ximo > 15s)")
        if len(failed_tests) > 0:
            print("   - Melhorar tratamento de erros e robustez")
        
        print(f"\n‚úÖ Teste conclu√≠do! {len(successful_tests)}/{len(results)} testes bem-sucedidos")

async def main():
    """Fun√ß√£o principal"""
    try:
        tester = PerformanceTester()
        await tester.test_performance_metrics()
        
    except Exception as e:
        print(f"‚ùå Erro durante os testes: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
